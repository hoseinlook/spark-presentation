{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0f49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.session import SparkSession , DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0b765",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "## Standalone\n",
    "![image](jupyter-files/standalone.png)\n",
    "## Kubernetes\n",
    "![image](jupyter-files/kuber.png)\n",
    "## Yarn\n",
    "\n",
    "![image](jupyter-files/yarn.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760af2e",
   "metadata": {},
   "source": [
    "# Make a SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8bfd1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Presentation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fab703e9a30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Presentation\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325c58",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feaac6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and create dataframes from diffrent file formats\n",
    "df_people = spark.read.json('data/people.json')\n",
    "df_sales = spark.read.csv('data/sales_info.csv',inferSchema=True,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05554ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Extra: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- PersonName: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "df_people.printSchema()\n",
    "df_sales.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb7f28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+\n",
      "|    Extra|age|   name|\n",
      "+---------+---+-------+\n",
      "|ExtraData| 25|Charlie|\n",
      "|     null| 30|  Frank|\n",
      "|     null| 26|   Tina|\n",
      "|     null| 30|    Amy|\n",
      "|     null| 28|   Carl|\n",
      "|     null| 19|   John|\n",
      "|     null| 17|Vanessa|\n",
      "|     null| 29|  Sarah|\n",
      "|     null| 23|  Linda|\n",
      "|     null| 21|   Mike|\n",
      "|     null| 22|  Chris|\n",
      "|     null| 24|    Sam|\n",
      "+---------+---+-------+\n",
      "\n",
      "+-------+----------+-----+\n",
      "|Company|PersonName|Sales|\n",
      "+-------+----------+-----+\n",
      "|   GOOG|       Sam|200.0|\n",
      "|   GOOG|   Charlie|120.0|\n",
      "|   GOOG|     Frank|340.0|\n",
      "|   MSFT|      Tina|600.0|\n",
      "|   MSFT|       Amy|124.0|\n",
      "|   MSFT|   Vanessa|243.0|\n",
      "|     FB|      Carl|870.0|\n",
      "|     FB|     Sarah|350.0|\n",
      "|   APPL|      John|250.0|\n",
      "|   APPL|     Linda|130.0|\n",
      "|   APPL|      Mike|750.0|\n",
      "|   APPL|     Chris|350.0|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "df_people.show()\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a812c",
   "metadata": {},
   "source": [
    "# Basics (Spark SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da545c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Extra: string, age: bigint, name: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce395842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa0142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Charlie|\n",
      "|  Frank|\n",
      "|   Tina|\n",
      "|    Amy|\n",
      "|   Carl|\n",
      "|   John|\n",
      "|Vanessa|\n",
      "|  Sarah|\n",
      "|  Linda|\n",
      "|   Mike|\n",
      "|  Chris|\n",
      "|    Sam|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check UI | what is a job? \n",
    "# df_people.select('name')\n",
    "df_people.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7b725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+-------+\n",
      "|    Extra|age|   name|new_col|\n",
      "+---------+---+-------+-------+\n",
      "|ExtraData| 25|Charlie|     25|\n",
      "|     null| 30|  Frank|     30|\n",
      "|     null| 26|   Tina|     26|\n",
      "|     null| 30|    Amy|     30|\n",
      "|     null| 28|   Carl|     28|\n",
      "|     null| 19|   John|     19|\n",
      "|     null| 17|Vanessa|     17|\n",
      "|     null| 29|  Sarah|     29|\n",
      "|     null| 23|  Linda|     23|\n",
      "|     null| 21|   Mike|     21|\n",
      "|     null| 22|  Chris|     22|\n",
      "|     null| 24|    Sam|     24|\n",
      "+---------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrames, like RDDs, are immutable\n",
    "df_people.withColumn(\"new_col\", f.col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1022ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+-------+-------+----------+-----+\n",
      "|    Extra|age|   name|new_col|Company|PersonName|Sales|\n",
      "+---------+---+-------+-------+-------+----------+-----+\n",
      "|ExtraData| 25|Charlie|     25|   GOOG|   Charlie|120.0|\n",
      "|     null| 30|  Frank|     30|   GOOG|     Frank|340.0|\n",
      "|     null| 26|   Tina|     26|   MSFT|      Tina|600.0|\n",
      "|     null| 30|    Amy|     30|   MSFT|       Amy|124.0|\n",
      "|     null| 28|   Carl|     28|     FB|      Carl|870.0|\n",
      "|     null| 19|   John|     19|   APPL|      John|250.0|\n",
      "|     null| 17|Vanessa|     17|   MSFT|   Vanessa|243.0|\n",
      "|     null| 29|  Sarah|     29|     FB|     Sarah|350.0|\n",
      "|     null| 23|  Linda|     23|   APPL|     Linda|130.0|\n",
      "|     null| 21|   Mike|     21|   APPL|      Mike|750.0|\n",
      "|     null| 24|    Sam|     24|   GOOG|       Sam|200.0|\n",
      "+---------+---+-------+-------+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark is lazy\n",
    "new_people_df = df_people.withColumn(\"new_col\", f.col(\"age\"))\n",
    "\n",
    "people_with_sales = new_people_df.join(df_sales, new_people_df.name==df_sales.PersonName)\n",
    "people_with_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dca8d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+-------+-------+-----+\n",
      "|    Extra|age|   name|new_col|Company|Sales|\n",
      "+---------+---+-------+-------+-------+-----+\n",
      "|ExtraData| 25|Charlie|     25|   GOOG|120.0|\n",
      "|     null| 30|  Frank|     30|   GOOG|340.0|\n",
      "|     null| 26|   Tina|     26|   MSFT|600.0|\n",
      "|     null| 30|    Amy|     30|   MSFT|124.0|\n",
      "|     null| 28|   Carl|     28|     FB|870.0|\n",
      "|     null| 19|   John|     19|   APPL|250.0|\n",
      "|     null| 17|Vanessa|     17|   MSFT|243.0|\n",
      "|     null| 29|  Sarah|     29|     FB|350.0|\n",
      "|     null| 23|  Linda|     23|   APPL|130.0|\n",
      "|     null| 21|   Mike|     21|   APPL|750.0|\n",
      "|     null| 24|    Sam|     24|   GOOG|200.0|\n",
      "+---------+---+-------+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_with_sales.drop(f.col(\"PersonName\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca0d22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1130.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy\n",
    "people_with_sales.groupBy(\"Company\").agg(f.sum(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2be445e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+-------+-------+----------+-----+\n",
      "|    Extra|age|   name|new_col|Company|PersonName|Sales|\n",
      "+---------+---+-------+-------+-------+----------+-----+\n",
      "|ExtraData| 25|Charlie|     25|   GOOG|   Charlie|120.0|\n",
      "|     null| 30|  Frank|     30|   GOOG|     Frank|340.0|\n",
      "|     null| 26|   Tina|     26|   MSFT|      Tina|600.0|\n",
      "|     null| 30|    Amy|     30|   MSFT|       Amy|124.0|\n",
      "|     null| 28|   Carl|     28|     FB|      Carl|870.0|\n",
      "|     null| 19|   John|     19|   APPL|      John|250.0|\n",
      "|     null| 17|Vanessa|     17|   MSFT|   Vanessa|243.0|\n",
      "|     null| 29|  Sarah|     29|     FB|     Sarah|350.0|\n",
      "|     null| 23|  Linda|     23|   APPL|     Linda|130.0|\n",
      "|     null| 21|   Mike|     21|   APPL|      Mike|750.0|\n",
      "|     null| 24|    Sam|     24|   GOOG|       Sam|200.0|\n",
      "+---------+---+-------+-------+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use sql syntax\n",
    "people_with_sales.createOrReplaceTempView(\"temp_table\")\n",
    "spark.sql(\"select * from temp_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48293",
   "metadata": {},
   "source": [
    "# Spark Mlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11177275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 20:58:32 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+---------+\n",
      "|label| features|\n",
      "+-----+---------+\n",
      "|  0.0|(3,[],[])|\n",
      "+-----+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/sample_kmeans_data.txt\")\n",
    "\n",
    "dataset.printSchema()\n",
    "dataset.show(1)\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ca8e1",
   "metadata": {},
   "source": [
    "# Datawarehousing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce081008",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_with_sales.write.partitionBy(\"Company\").mode(\"overwrite\").json(\"output/tables/people_with_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efd444",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "![image](jupyter-files/streaming2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e31f08",
   "metadata": {},
   "source": [
    "### WordCount | SparkStreaming with windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf96b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 3)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "wordCounts.window(6).pprint()\n",
    "\n",
    "print(type(lines))\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7373f5",
   "metadata": {},
   "source": [
    "## SparkStructuredStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "# `nc -lk 9999`\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"checkpointLocation\",\"checkpoints\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   f.explode(\n",
    "       f.split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "    .format(\"console\")\n",
    "query.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f95cf1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301af81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a6dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac53a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a21ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2c0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-presentation",
   "language": "python",
   "name": "spark-presentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
