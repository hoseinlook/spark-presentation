{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0f49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.session import SparkSession , DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0b765",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "## Standalone\n",
    "![image](jupyter-files/standalone.png)\n",
    "## Kubernetes\n",
    "![image](jupyter-files/kuber.png)\n",
    "## Yarn\n",
    "\n",
    "![image](jupyter-files/yarn.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760af2e",
   "metadata": {},
   "source": [
    "# Make a SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bfd1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 20:27:59 WARN Utils: Your hostname, hoseins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.102 instead (on interface en0)\n",
      "23/05/19 20:27:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/19 20:27:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Presentation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7b50404d60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Presentation\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325c58",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaac6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and create dataframes from diffrent file formats\n",
    "df_people = spark.read.json('data/people.json')\n",
    "df_sales = spark.read.csv('data/sales_info.csv',inferSchema=True,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05554ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema\n",
    "df_people.printSchema()\n",
    "df_sales.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema\n",
    "df_people.show()\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a812c",
   "metadata": {},
   "source": [
    "# Basics (Spark SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da545c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce395842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check UI | what is a job? \n",
    "# df_people.select('name')\n",
    "df_people.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames, like RDDs, are immutable\n",
    "df_people.withColumn(\"new_col\", f.col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1022ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark is lazy\n",
    "new_people_df = df_people.withColumn(\"new_col\", f.col(\"age\"))\n",
    "\n",
    "people_with_sales = new_people_df.join(df_sales, new_people_df.name==df_sales.PersonName)\n",
    "people_with_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_with_sales.drop(f.col(\"PersonName\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy\n",
    "people_with_sales.groupBy(\"Company\").agg(f.sum(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be445e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sql syntax\n",
    "people_with_sales.createOrReplaceTempView(\"temp_table\")\n",
    "spark.sql(\"select * from temp_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48293",
   "metadata": {},
   "source": [
    "# Spark Mlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11177275",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Loads data.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibsvm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/sample_kmeans_data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/uni/spark-presentation/venv/lib/python3.9/site-packages/pyspark/ml/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one or more\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# contributor license agreements.  See the NOTICE file distributed with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mDataFrame-based machine learning APIs to let users quickly assemble and configure practical\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mmachine learning pipelines.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     Estimator,\n\u001b[1;32m     24\u001b[0m     Model,\n\u001b[1;32m     25\u001b[0m     Predictor,\n\u001b[1;32m     26\u001b[0m     PredictionModel,\n\u001b[1;32m     27\u001b[0m     Transformer,\n\u001b[1;32m     28\u001b[0m     UnaryTransformer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     classification,\n\u001b[1;32m     33\u001b[0m     clustering,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     param,\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/uni/spark-presentation/venv/lib/python3.9/site-packages/pyspark/ml/base.py:40\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     Any,\n\u001b[1;32m     25\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m since\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m P\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inherit_doc\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     HasInputCol,\n\u001b[1;32m     44\u001b[0m     HasOutputCol,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     Params,\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/uni/spark-presentation/venv/lib/python3.9/site-packages/pyspark/ml/param/__init__.py:32\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     Any,\n\u001b[1;32m     22\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JavaObject\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseVector, Vector, Matrix\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efd444",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "![image](jupyter-files/streaming2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e31f08",
   "metadata": {},
   "source": [
    "### WordCount | SparkStreaming with windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf96b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 3)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "wordCounts.window(6).pprint()\n",
    "\n",
    "print(type(lines))\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7373f5",
   "metadata": {},
   "source": [
    "## SparkStructuredStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "# `nc -lk 9999`\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"checkpointLocation\",\"checkpoints\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   f.explode(\n",
    "       f.split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "    .format(\"console\")\n",
    "query.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f95cf1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301af81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a6dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac53a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a21ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2c0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-presentation",
   "language": "python",
   "name": "spark-presentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
